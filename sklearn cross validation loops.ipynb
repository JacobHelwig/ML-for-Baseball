{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_concat = pd.read_csv('2018master.csv') # read in training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "teams_concat_test = pd.read_csv('2019master.csv') # read in test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide training set into predictors/ responses\n",
    "X_train = teams_concat.drop([\"date\", 'Opposing Team', 'Opposing Starter', 'Opposing Team Score',\n",
    "                      'Won','Runs Scored'], 1)\n",
    "\n",
    "y_train_reg = teams_concat['Runs Scored']\n",
    "y_train_cl = teams_concat['Won']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# divide test set into predictors/responses\n",
    "X_test = teams_concat_test.drop([\"Team\", \"date\", 'Opposing Team', 'Opposing Starter', 'Opposing Team Score',\n",
    "                      'Won','Runs Scored'], 1)\n",
    "y_test_reg = teams_concat_test['Runs Scored']\n",
    "y_test_cl = teams_concat_test[\"Won\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Standardization to perform PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Variance in each PC: [3.19463866e-01 9.63268787e-02 7.46201155e-02 6.40509923e-02\n",
      " 4.52214858e-02 4.50956068e-02 3.88045521e-02 3.32196487e-02\n",
      " 2.97387695e-02 2.82373116e-02 2.63409699e-02 2.54123800e-02\n",
      " 2.42966740e-02 2.42029234e-02 2.02622032e-02 1.93730565e-02\n",
      " 1.59165422e-02 1.39463116e-02 1.27015518e-02 1.10029511e-02\n",
      " 7.69954565e-03 5.57751057e-03 5.24920049e-03 3.24578213e-03\n",
      " 2.75310755e-03 2.37309431e-03 2.27159985e-03 1.33391157e-03\n",
      " 1.03888268e-03 1.58992185e-04 5.34878333e-05 5.48109328e-06\n",
      " 2.42738691e-06 1.03619630e-06 8.43401062e-07 2.93990810e-07\n",
      " 1.29166543e-08 6.30031386e-32]\n"
     ]
    }
   ],
   "source": [
    "# want to do it on the scaled data set\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_scaled = scaler.transform(X_train)\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA()\n",
    "pca_data = pca.fit_transform(X_scaled)\n",
    "pca_data_df = pd.DataFrame(pca_data)\n",
    "\n",
    "var_exp = pca.explained_variance_ratio_\n",
    "print(\"Variance in each PC:\", var_exp) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7465419149289534"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cum_var_exp[8]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold CV for decision trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 182 candidates, totalling 1820 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 4, 'max_features': 28}\n",
      "RMSE:  -9.825520535694864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 1820 out of 1820 | elapsed:   45.5s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "params = {\"max_depth\": list(range(1,41,3)), \"max_features\":list(range(1,39,3))}\n",
    "\n",
    "grid_search = GridSearchCV(dtr, params, cv=10, scoring='neg_mean_squared_error', verbose = 1)\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(\"RMSE: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 99 candidates, totalling 9900 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'max_features': 33}\n",
      "RMSE:  -9.743448167582686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 9900 out of 9900 | elapsed:  3.0min finished\n",
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "params = {\"max_depth\": list(range(1,10)), \"max_features\":list(range(23,34))}\n",
    "\n",
    "# Create the GridSearchCV object:\n",
    "grid_search = GridSearchCV(dtr, params, cv=100, scoring='neg_mean_squared_error', verbose = 1)\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(\"RMSE: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 1000 folds for each of 36 candidates, totalling 36000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'max_depth': 3, 'max_features': 31}\n",
      "RMSE:  -9.780283395836287\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 36000 out of 36000 | elapsed:  9.9min finished\n",
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "dtr = DecisionTreeRegressor()\n",
    "\n",
    "params = {\"max_depth\": list(range(1,5)), \"max_features\":list(range(30,39))}\n",
    "\n",
    "# Create the GridSearchCV object:\n",
    "grid_search = GridSearchCV(dtr, params, cv=1000, scoring='neg_mean_squared_error', verbose = 1)\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_params_)\n",
    "print(\"RMSE: \", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross Validation For SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from sklearn.svm import SVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 48 candidates, totalling 144 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 144 out of 144 | elapsed: 24.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5740436034553682\n",
      "{'SVM__C': 80, 'SVM__kernel': 'linear', 'pca__n_components': 23}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('SVM', svc)])\n",
    "\n",
    "param_grid = {\"SVM__kernel\":['linear', 'rbf', 'poly', 'sigmoid'], \n",
    "              \"SVM__C\":list(range(40,101,20)), \"pca__n_components\":[23,25,27]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 1)\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 45 candidates, totalling 450 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 450 out of 450 | elapsed: 46.3min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6034553681612506\n",
      "{'SVM__C': 90, 'SVM__kernel': 'sigmoid', 'pca__n_components': 8}\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "\n",
    "svc = SVC()\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('SVM', svc)])\n",
    "\n",
    "param_grid = {\"SVM__kernel\":['linear', 'rbf','sigmoid'], \n",
    "              \"SVM__C\":list(range(70,100,10)), \"pca__n_components\":list(range(3,24,5))}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, scoring = 'accuracy', verbose = 1)\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 24 candidates, totalling 240 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 240 out of 240 | elapsed: 21.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6013986013986014\n",
      "{'SVM__C': 90, 'SVM__kernel': 'sigmoid', 'pca__n_components': 8}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "pca = PCA()\n",
    "scaler = StandardScaler()\n",
    "svc = SVC()\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('SVM', svc)])\n",
    "\n",
    "param_grid = {\"SVM__C\":list(range(85,101,5)), \"pca__n_components\":list(range(7,10)), \"SVM__kernel\":[\"sigmoid\", \"linear\"]}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, scoring = 'accuracy', verbose = 1)\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Loop with a tiny cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 9 candidates, totalling 90 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done  90 out of  90 | elapsed:  2.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6217605923488276\n",
      "{'SVM__kernel': 'sigmoid', 'pca__n_components': 9}\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "\n",
    "svc = SVC(.01)\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('SVM', svc)])\n",
    "\n",
    "param_grid = {\"pca__n_components\":list(range(3,10,3)), \"SVM__kernel\":['sigmoid','linear','rbf']}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, scoring = 'accuracy', verbose = 1)\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 30 folds for each of 14 candidates, totalling 420 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 420 out of 420 | elapsed: 13.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6213492389962978\n",
      "{'SVM__kernel': 'sigmoid', 'pca__n_components': 6}\n"
     ]
    }
   ],
   "source": [
    "pca = PCA()\n",
    "svc = SVC(.01)\n",
    "\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('SVM', svc)])\n",
    "\n",
    "param_grid = {\"pca__n_components\":list(range(6,13)), \"SVM__kernel\":['sigmoid','linear']}\n",
    "\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 30, scoring = 'accuracy', verbose = 1)\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV loops to find ideal number of SVMs to ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor # multilayer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 6 candidates, totalling 18 fits\n",
      "[CV] BaggingClassifier__n_estimators=100 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    8.0s finished\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.5min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=100, score=0.603, total= 1.5min\n",
      "[CV] BaggingClassifier__n_estimators=100 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.9s finished\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  3.0min remaining:    0.0s\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=100, score=0.582, total= 1.5min\n",
      "[CV] BaggingClassifier__n_estimators=100 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    7.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=100, score=0.575, total= 1.5min\n",
      "[CV] BaggingClassifier__n_estimators=300 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  3.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   23.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=300, score=0.601, total= 4.3min\n",
      "[CV] BaggingClassifier__n_estimators=300 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   22.6s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=300, score=0.581, total= 4.5min\n",
      "[CV] BaggingClassifier__n_estimators=300 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  4.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   22.2s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=300, score=0.577, total= 4.5min\n",
      "[CV] BaggingClassifier__n_estimators=500 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   40.5s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=500, score=0.602, total= 7.5min\n",
      "[CV] BaggingClassifier__n_estimators=500 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   39.1s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=500, score=0.581, total= 7.8min\n",
      "[CV] BaggingClassifier__n_estimators=500 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  7.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   36.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=500, score=0.577, total= 7.7min\n",
      "[CV] BaggingClassifier__n_estimators=700 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  9.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   53.0s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=700, score=0.602, total=10.2min\n",
      "[CV] BaggingClassifier__n_estimators=700 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  9.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   55.8s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=700, score=0.584, total=10.3min\n",
      "[CV] BaggingClassifier__n_estimators=700 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  9.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   49.4s finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=700, score=0.577, total=10.3min\n",
      "[CV] BaggingClassifier__n_estimators=900 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 59.5min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.2min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=900, score=0.602, total=60.7min\n",
      "[CV] BaggingClassifier__n_estimators=900 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 12.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=900, score=0.582, total=13.4min\n",
      "[CV] BaggingClassifier__n_estimators=900 .............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 11.8min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] . BaggingClassifier__n_estimators=900, score=0.578, total=13.0min\n",
      "[CV] BaggingClassifier__n_estimators=1100 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 14.1min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  BaggingClassifier__n_estimators=1100, score=0.602, total=15.5min\n",
      "[CV] BaggingClassifier__n_estimators=1100 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 14.7min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  BaggingClassifier__n_estimators=1100, score=0.581, total=16.0min\n",
      "[CV] BaggingClassifier__n_estimators=1100 ............................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 14.4min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.3min finished\n",
      "[Parallel(n_jobs=1)]: Done  18 out of  18 | elapsed: 205.9min finished\n",
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  BaggingClassifier__n_estimators=1100, score=0.578, total=15.7min\n",
      "0.5878239407651172\n",
      "{'BaggingClassifier__n_estimators': 700}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed: 23.1min finished\n"
     ]
    }
   ],
   "source": [
    "#create a PCA\n",
    "pca = PCA(n_components = 8)\n",
    "\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "clf = BaggingClassifier(base_estimator=SVC(kernel = \"sigmoid\", C = .01, probability = True),verbose = True)\n",
    "\n",
    "#create a pipeline that does scaling, then PCA, then SVM\n",
    "pipe = Pipeline(steps=[('scaler', scaler), ('pca', pca), ('BaggingClassifier', clf)])\n",
    "\n",
    "#Set up the parameters you want to tune for each of your pipeline steps\n",
    "#Parameters of pipelines can be set using ‘__’ separated parameter names:\n",
    "param_grid = {\"BaggingClassifier__n_estimators\":list(range(100,1101, 200))}\n",
    "\n",
    "# pass the pipeline and the parameters into a GridSearchCV with a 3-fold cross validation\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 3, scoring = 'accuracy', verbose = 3)\n",
    "# call fit() on the GridSearchCV and pass in the unscaled data (X_values, Y_values)\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "# print out the best_score_ and best_params_ from the GridSearchCV\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested CV loop for neural net with one layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 140 candidates, totalling 1400 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1400 out of 1400 | elapsed: 27.6min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.51963933809116\n",
      "{'MLPRegressor__activation': 'logistic', 'MLPRegressor__hidden_layer_sizes': (50,), 'pca__n_components': 6}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "\n",
    "MLP = MLPRegressor(early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":[(10,),(30,),(50,),(70,),(90,)], \n",
    "              'pca__n_components':list(range(1,32,5)), \"MLPRegressor__activation\":['identity','logistic', 'tanh', 'relu']}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 180 candidates, totalling 1800 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 1800 out of 1800 | elapsed: 33.5min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.512420666681672\n",
      "{'MLPRegressor__activation': 'logistic', 'MLPRegressor__hidden_layer_sizes': 60, 'pca__n_components': 9}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "pca = PCA()\n",
    "MLP = MLPRegressor(early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":list(range(30,71,5)),\n",
    "              'pca__n_components':list(range(1,11,2)), \"MLPRegressor__activation\":['identity','logistic', 'tanh', 'relu']}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 100 candidates, totalling 2000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 2000 out of 2000 | elapsed: 32.2min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.517876072921288\n",
      "{'MLPRegressor__hidden_layer_sizes': 54, 'pca__n_components': 8}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "MLP = MLPRegressor(early_stopping = True, max_iter = 1000, learning_rate = 'adaptive', activation = 'logistic')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":list(range(50,70)),\n",
    "              'pca__n_components':list(range(7,12))}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 20, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nested CV for a 2 layer net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "widths = list(range(10,101,30))\n",
    "\n",
    "tuple_list = []\n",
    "for i in (range (0, len(widths))):\n",
    "  elem1 = widths[i]\n",
    "  for j in range(0, len(widths)):\n",
    "    elem2 = widths[j]\n",
    "    tuple_list.append((elem1, elem2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 320 candidates, totalling 3200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 3200 out of 3200 | elapsed: 76.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.520054154016817\n",
      "{'MLPRegressor__activation': 'identity', 'MLPRegressor__hidden_layer_sizes': (40, 100), 'pca__n_components': 8}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "MLP = MLPRegressor(early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":tuple_list,\n",
    "              'pca__n_components':list(range(3,24,5)), \"MLPRegressor__activation\":['identity','logistic', 'tanh', 'relu']}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "width1 = list(range(20,61,5))\n",
    "width2 = list(range(80,121,5))\n",
    "tuple_list = []\n",
    "for i in (range (0, len(width1))):\n",
    "  elem1 = width1[i]\n",
    "  for j in range(0, len(width2)):\n",
    "    elem2 = width2[j]\n",
    "    tuple_list.append((elem1, elem2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 1620 candidates, totalling 16200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 16200 out of 16200 | elapsed: 366.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.518032194398229\n",
      "{'MLPRegressor__activation': 'identity', 'MLPRegressor__hidden_layer_sizes': (25, 80), 'pca__n_components': 6}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "MLP = MLPRegressor(early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":tuple_list,\n",
    "              'pca__n_components':list(range(4,13,2)), \"MLPRegressor__activation\":['identity','logistic', 'tanh', 'relu']}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "width1 = list(range(15,36,5))\n",
    "width2 = list(range(40,86,5))\n",
    "tuple_list = []\n",
    "for i in (range (0, len(width1))):\n",
    "  elem1 = width1[i]\n",
    "  for j in range(0, len(width2)):\n",
    "    elem2 = width2[j]\n",
    "    tuple_list.append((elem1, elem2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 20 folds for each of 200 candidates, totalling 4000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 4000 out of 4000 | elapsed: 24.8min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.522704724069866\n",
      "{'MLPRegressor__hidden_layer_sizes': (15, 85), 'pca__n_components': 7}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "MLP = MLPRegressor(activation = 'identity',early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":tuple_list,\n",
    "              'pca__n_components':[7,8,9,10]}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 20, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No improvement in CV error. Stop validating"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CV Loop for a three layer net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "width1 = list(range(30,91,30))\n",
    "width2 = list(range(30,91,30))\n",
    "width3 = list(range(30,91,30))\n",
    "tuple_list = []\n",
    "for i in (range (0, len(width1))):\n",
    "  elem1 = width1[i]\n",
    "  for j in range(0, len(width2)):\n",
    "    elem2 = width2[j]\n",
    "    for k in range(len(width3)):\n",
    "      elem3 = width3[k]  \n",
    "      tuple_list.append((elem1, elem2, elem3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 108 candidates, totalling 324 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 324 out of 324 | elapsed:  7.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.51946177356567\n",
      "{'MLPRegressor__activation': 'identity', 'MLPRegressor__hidden_layer_sizes': (60, 60, 60)}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA(9)\n",
    "\n",
    "MLP = MLPRegressor(early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":tuple_list, \"MLPRegressor__activation\":['identity','logistic', 'tanh', 'relu']}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 3, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "width1 = list(range(40,81,20))\n",
    "width2 = list(range(40,81,20))\n",
    "width3 = list(range(40,81,20))\n",
    "tuple_list = []\n",
    "for i in (range (0, len(width1))):\n",
    "  elem1 = width1[i]\n",
    "  for j in range(0, len(width2)):\n",
    "    elem2 = width2[j]\n",
    "    for k in range(len(width3)):\n",
    "      elem3 = width3[k]  \n",
    "      tuple_list.append((elem1, elem2, elem3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 54 candidates, totalling 270 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 270 out of 270 | elapsed:  8.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.538895591900763\n",
      "{'MLPRegressor__activation': 'identity', 'MLPRegressor__hidden_layer_sizes': (80, 40, 80)}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA(9)\n",
    "\n",
    "MLP = MLPRegressor(early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":tuple_list, \"MLPRegressor__activation\":['identity','logistic']}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 5, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "width1 = list(range(60,121,20))\n",
    "width2 = list(range(10,71,20))\n",
    "width3 = list(range(60,121,20))\n",
    "tuple_list = []\n",
    "for i in (range (0, len(width1))):\n",
    "  elem1 = width1[i]\n",
    "  for j in range(0, len(width2)):\n",
    "    elem2 = width2[j]\n",
    "    for k in range(len(width3)):\n",
    "      elem3 = width3[k]  \n",
    "      tuple_list.append((elem1, elem2, elem3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 128 candidates, totalling 640 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 640 out of 640 | elapsed: 19.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.524704806638795\n",
      "{'MLPRegressor__activation': 'identity', 'MLPRegressor__hidden_layer_sizes': (60, 30, 120)}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA(9)\n",
    "\n",
    "MLP = MLPRegressor(early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":tuple_list, \"MLPRegressor__activation\":['identity','logistic']}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 5, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "width1 = list(range(30,91,10))\n",
    "width2 = list(range(10,51,10))\n",
    "width3 = list(range(100,140,10))\n",
    "tuple_list = []\n",
    "for i in (range (0, len(width1))):\n",
    "  elem1 = width1[i]\n",
    "  for j in range(0, len(width2)):\n",
    "    elem2 = width2[j]\n",
    "    for k in range(len(width3)):\n",
    "      elem3 = width3[k]  \n",
    "      tuple_list.append((elem1, elem2, elem3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 140 candidates, totalling 700 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 700 out of 700 | elapsed:  7.0min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.517989917879966\n",
      "{'MLPRegressor__hidden_layer_sizes': (30, 30, 100)}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA(9)\n",
    "\n",
    "MLP = MLPRegressor(activation = 'identity', early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":tuple_list}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 5, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "width1 = list(range(10,51,10))\n",
    "width2 = list(range(10,51,10))\n",
    "width3 = list(range(80,121,10))\n",
    "tuple_list = []\n",
    "for i in (range (0, len(width1))):\n",
    "  elem1 = width1[i]\n",
    "  for j in range(0, len(width2)):\n",
    "    elem2 = width2[j]\n",
    "    for k in range(len(width3)):\n",
    "      elem3 = width3[k]  \n",
    "      tuple_list.append((elem1, elem2, elem3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 125 candidates, totalling 625 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 625 out of 625 | elapsed:  5.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-9.522853280848597\n",
      "{'MLPRegressor__hidden_layer_sizes': (20, 30, 110)}\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA(9)\n",
    "\n",
    "MLP = MLPRegressor(activation = 'identity', early_stopping = True, max_iter = 1000, learning_rate = 'adaptive')\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"MLPRegressor\", MLP)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"MLPRegressor__hidden_layer_sizes\":tuple_list}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 5, verbose = 1, scoring = 'neg_mean_squared_error')\n",
    "\n",
    "grid_search.fit(X_train, y_train_reg)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No increase in CV error; end tuning here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K- Nearest Neighbors CV Loops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 20 candidates, totalling 200 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5619086795557384\n",
      "{'KNeighborsClassifier__n_neighbors': 2000, 'pca__n_components': 1}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 200 out of 200 | elapsed:   42.2s finished\n"
     ]
    }
   ],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"KNeighborsClassifier\", knc)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"KNeighborsClassifier__n_neighbors\":list(range(500,2001,500)), \"pca__n_components\":list(range(1,26,5))}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, verbose = 1, scoring = 'accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 30 candidates, totalling 300 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5715754833401893\n",
      "{'KNeighborsClassifier__n_neighbors': 2500, 'pca__n_components': 3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 300 out of 300 | elapsed:  1.5min finished\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"KNeighborsClassifier\", knc)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"KNeighborsClassifier__n_neighbors\":list(range(1500,2501,250)), \"pca__n_components\":list(range(1,12,2))}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, verbose = 1, scoring = 'accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 25 candidates, totalling 250 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5771287535993418\n",
      "{'KNeighborsClassifier__n_neighbors': 3250, 'pca__n_components': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 250 out of 250 | elapsed:  1.3min finished\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"KNeighborsClassifier\", knc)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"KNeighborsClassifier__n_neighbors\":list(range(2250,3251,250)), \"pca__n_components\":list(range(1,6))}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, verbose = 1, scoring = 'accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 10 folds for each of 36 candidates, totalling 360 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5785684903331962\n",
      "{'KNeighborsClassifier__n_neighbors': 3300, 'pca__n_components': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 360 out of 360 | elapsed:  2.2min finished\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"KNeighborsClassifier\", knc)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"KNeighborsClassifier__n_neighbors\":list(range(3000,3501,100)), \"pca__n_components\":list(range(2,8))}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 10, verbose = 1, scoring = 'accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 100 folds for each of 120 candidates, totalling 12000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5682846565199506\n",
      "{'KNeighborsClassifier__n_neighbors': 3275, 'pca__n_components': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done 12000 out of 12000 | elapsed: 17.5min finished\n",
      "C:\\Users\\jacob\\Anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:814: DeprecationWarning: The default of the `iid` parameter will change from True to False in version 0.22 and will be removed in 0.24. This will change numeric results when test-set sizes are unequal.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "\n",
    "pca = PCA()\n",
    "\n",
    "knc = KNeighborsClassifier()\n",
    "\n",
    "step = [('scaler', scaler),('pca', pca),(\"KNeighborsClassifier\", knc)]\n",
    "pipe = Pipeline(step)\n",
    "param_grid = {\"KNeighborsClassifier__n_neighbors\":list(range(3200,3400,5)), \"pca__n_components\":list(range(4,7))}\n",
    "grid_search = GridSearchCV(pipe, param_grid, cv = 100, verbose = 1, scoring = 'accuracy')\n",
    "\n",
    "grid_search.fit(X_train, y_train_cl)\n",
    "\n",
    "print(grid_search.best_score_)\n",
    "print(grid_search.best_params_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
